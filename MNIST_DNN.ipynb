{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "###### Do not modify here ###### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters setting\n",
    "dim = 28 * 28\n",
    "learning_rate = 0.01\n",
    "num_steps = 100\n",
    "batch_size = 2048\n",
    "num_classes = 5 # 0-4\n",
    "early_stop = 20\n",
    "dropout = 0.5 \n",
    "\n",
    "#################################### Build Execution Graph ###########################################\n",
    "# Dataset definition\n",
    "X = tf.placeholder(tf.float32, shape = (None, dim))\n",
    "y = tf.placeholder(tf.int64, shape = (None))\n",
    "\n",
    "# HW Hint\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "adam = tf.train.AdamOptimizer(learning_rate)\n",
    "activation = tf.nn.elu\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Build DNN model with five hidden layers of 128 neurons\n",
    "hidden_layer = tf.layers.dense(X, 128, activation=activation, kernel_initializer=initializer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "hidden_layer = tf.layers.dense(hidden_layer, 128, activation=activation, kernel_initializer=initializer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "hidden_layer = tf.layers.dense(hidden_layer, 128, activation=activation, kernel_initializer=initializer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "hidden_layer = tf.layers.dense(hidden_layer, 128, activation=activation, kernel_initializer=initializer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "hidden_layer = tf.layers.dense(hidden_layer, 128, activation=activation, kernel_initializer=initializer)\n",
    "# linear activation\n",
    "out_layer = tf.layers.dense(hidden_layer, num_classes, kernel_initializer=initializer)\n",
    "\n",
    "# Softmax\n",
    "softY = tf.nn.softmax(out_layer)\n",
    "\n",
    "# Compute loss\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits : combine softmax and cross_entropy\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out_layer)\n",
    "loss = tf.reduce_mean(loss)\n",
    "optimizer = adam.minimize(loss)\n",
    "\n",
    "# Compute accuracy\n",
    "predlabelY = tf.nn.top_k(softY, 1).indices # label 0-4\n",
    "predY = tf.nn.in_top_k(softY, y, 1) # boolean\n",
    "accuracy = tf.reduce_mean(tf.cast(predY, tf.float32)) # boolean -> 0/1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "###########################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### N-fold Cross Validaion ###########################################\n",
    "N = 10\n",
    "kf = KFold(n_splits=N)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Combine training and validation datasets\n",
    "X_input = np.concatenate((X_train1, X_valid1), axis=0)\n",
    "Y_input = np.concatenate((y_train1, y_valid1), axis=0)\n",
    "\n",
    "print X_input.shape\n",
    "print Y_input.shape\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    \n",
    "    precision_set = np.zeros(num_classes)\n",
    "    recall_set = np.zeros(num_classes)\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X_input, Y_input):\n",
    "        # Early-stop initialization\n",
    "        best_loss = np.infty\n",
    "        best_acc = 0\n",
    "        patience = 0\n",
    "        # Use id from kf(KFold) to get training and valudation data in each fold \n",
    "        trainX = X_input[train_idx]\n",
    "        trainY = Y_input[train_idx]\n",
    "        validationX = X_input[val_idx]\n",
    "        validationY = Y_input[val_idx]\n",
    "        \n",
    "        num_batch = int(len(trainX) / batch_size) + 1\n",
    "        \n",
    "        for epoch in range(num_steps):\n",
    "            # shuffle the data\n",
    "            rd_idx = range(len(trainX))\n",
    "            np.random.shuffle(rd_idx)\n",
    "\n",
    "            # array_split : array , sections\n",
    "            for idx in np.array_split(rd_idx, num_batch):\n",
    "                batchX = trainX[idx]\n",
    "                batchY = trainY[idx]\n",
    "                sess.run(optimizer, feed_dict={ X: batchX, y: batchY, keep_prob: dropout })\n",
    "\n",
    "            # Calculate the validation loss and accuracy\n",
    "            val_loss, val_acc = sess.run([loss, accuracy], feed_dict={ X: validationX, y: validationY, keep_prob: 1.0 })\n",
    "            estimatedY, estimatedlabelY = sess.run([predY, predlabelY], feed_dict={ X: validationX, y: validationY, keep_prob: 1.0 })\n",
    "            \n",
    "            # Calculate the precision and recall \n",
    "            precision = precision_score(validationY, estimatedlabelY, average=None)\n",
    "            recall = recall_score(validationY, estimatedlabelY, average=None)\n",
    "            \n",
    "            # Check if loss is less than the best loss\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                patience = 0\n",
    "                best_precision = precision\n",
    "                best_recall = recall\n",
    "                # Save the current network to a checkpoint file\n",
    "                best_checkpoint = saver.save(sess, \"./Team60_HW2_\"+str(fold)+\".ckpt\")\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "                if patience > early_stop:\n",
    "                    print \"Early stop!\"\n",
    "                    break\n",
    "                    \n",
    "        loss_list.append(best_loss)\n",
    "        acc_list.append(best_acc)\n",
    "        precision_set = precision_set + best_precision\n",
    "        recall_set = recall_set + best_recall\n",
    "        print(\"Fold_N: {} Best_loss: {:.4f} Best_acc: {:.2f}\".format(fold, best_loss, best_acc)) \n",
    "        fold = fold + 1\n",
    "        \n",
    "    # Average all n-fold models   \n",
    "    print(\"Cross validation : loss {:.2f} accuracy {:.2f}\".format(np.mean(loss_list), np.mean(acc_list))) \n",
    "    \n",
    "    print \"----------------------------\"\n",
    "    print \"label precision recall\"\n",
    "  \n",
    "    for i in range(num_classes):\n",
    "        print(\"  {}     {:.2f}     {:.2f}\".format(i, precision_set[i] / N, recall_set[i] / N))\n",
    "    \n",
    "    print \"----------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### Training ###########################################\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "###### Start TF session ######\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Early-stop initialization\n",
    "    best_loss = np.infty\n",
    "    best_acc = 0\n",
    "    patience = 0\n",
    "    \n",
    "    num_batch = int(len(X_train1) / batch_size) + 1\n",
    "    for epoch in range(num_steps):\n",
    "            #shuffle\n",
    "            rd_idx = range(len(X_train1))\n",
    "            np.random.shuffle(rd_idx)\n",
    "\n",
    "            # array_split : array , sections \n",
    "            for idx in np.array_split(rd_idx, num_batch):\n",
    "                batchX = X_train1[idx]\n",
    "                batchY = y_train1[idx]\n",
    "                sess.run(optimizer, feed_dict={ X: batchX, y: batchY, keep_prob: dropout })\n",
    "\n",
    "            val_loss, val_acc = sess.run([loss, accuracy], feed_dict={ X: X_valid1, y: y_valid1, keep_prob: 1.0 })\n",
    "            estimatedY, estimatedlabelY = sess.run([predY, predlabelY], feed_dict={ X: X_valid1, y: y_valid1, keep_prob: 1.0 })\n",
    "            \n",
    "            precision = precision_score(y_valid1, estimatedlabelY, average=None)\n",
    "            recall = recall_score(y_valid1, estimatedlabelY, average=None)\n",
    "            \n",
    "            print val_acc\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_precision = precision\n",
    "                best_recall = recall\n",
    "                patience = 0\n",
    "                best_checkpoint = saver.save(sess, \"./Team60_HW2.ckpt\")\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "                if patience > early_stop:\n",
    "                    print \"Early stop!\"\n",
    "                    break\n",
    "                    \n",
    "    print(\"Best accuracy {:.2f}\".format(best_acc*100)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"----------------------------\"\n",
    "    print \"label precision recall\"\n",
    "  \n",
    "    for i in range(num_classes):\n",
    "        print(\"  {}     {:.2f}     {:.2f}\".format(i, best_precision[i], best_recall[i]))\n",
    "    \n",
    "    print \"----------------------------\"\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################### Testing ###########################################\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"./Team60_HW2.ckpt\")\n",
    "\n",
    "    val_loss, val_acc = sess.run([loss, accuracy], feed_dict={ X: X_test1, y: y_test1, keep_prob: 1.0 })\n",
    "    estimatedY, estimatedlabelY = sess.run([predY, predlabelY], feed_dict={ X: X_test1, y: y_test1, keep_prob: 1.0 })\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(val_acc * 100))\n",
    "    \n",
    "    \n",
    "    precision = precision_score(y_test1, estimatedlabelY, average=None)\n",
    "    recall = recall_score(y_test1, estimatedlabelY, average=None)\n",
    "    \n",
    "    print \"----------------------------\"\n",
    "    print \"label precision recall\"\n",
    "  \n",
    "    for i in range(num_classes):\n",
    "        print(\"  {}     {:.2f}     {:.2f}\".format(i, precision[i], recall[i]))\n",
    "    \n",
    "    print \"----------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
